{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary Python libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "#import plotly.offline as pyoff\n",
    "import plotly.graph_objs as go \n",
    "#import plotly.figure_factory as ff\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "# avoid displaying warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import dill\n",
    "import csv\n",
    "#import machine learning related libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "import time \n",
    "order_df ,order_df_split_1m,order_df_original= None,None,None\n",
    "order_df_with_all_data = None\n",
    "submission_df = None\n",
    "frequency_knn_cluster,recency_knn_cluster,quantity_knn_cluster = None,None,None\n",
    "frequency_knn_cluster_item,recency_knn_cluster_item,quantity_knn_cluster_item = {},{},{}\n",
    "dataframe_items_clusters_frequency = {}\n",
    "dataframe_items_clusters_recency = {}\n",
    "dataframe_items_clusters_quantity = {}\n",
    "dataframe_items = {}\n",
    "datafrane_items_realdataframe={}\n",
    "models = []\n",
    "models.append((\"LR\",LogisticRegression()))\n",
    "models.append((\"NB\",GaussianNB()))\n",
    "models.append((\"RF\",RandomForestClassifier()))\n",
    "models.append((\"SVC\",SVC()))\n",
    "models.append((\"Dtree\",DecisionTreeClassifier()))\n",
    "models.append((\"XGB\",xgb.XGBClassifier(enable_categorical=True)))\n",
    "models.append((\"KNN\",KNeighborsClassifier()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addRecency(order_df_complete):\n",
    "    global dataframe_items_clusters_recency\n",
    "    [order_df,item] = order_df_complete\n",
    "    ctm_max_purchase = order_df.groupby('userID').date.max().reset_index()\n",
    "    ctm_max_purchase.columns = ['userID','MaxPurchaseDate']\n",
    "    ctm_max_purchase['Recency'] = (ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n",
    "    number_of_clusters = 3\n",
    "    recency_knn_cluster = KMeans(n_clusters=number_of_clusters)\n",
    "    recency_knn_cluster.fit(ctm_max_purchase[['Recency']])\n",
    "    ctm_max_purchase['RecencyCluster'] = recency_knn_cluster.predict(ctm_max_purchase[['Recency']])\n",
    "    order_df = pd.merge(order_df, ctm_max_purchase[['userID', 'RecencyCluster']], on='userID')\n",
    "    dataframe_items_clusters_recency[item] = recency_knn_cluster\n",
    "    return [order_df,item]\n",
    "def addCompleteRecency(order_df):\n",
    "    ctm_max_purchase = order_df.groupby('userID').date.max().reset_index()\n",
    "    ctm_max_purchase.columns = ['userID','MaxPurchaseDate']\n",
    "    ctm_max_purchase['Recency'] = (ctm_max_purchase['MaxPurchaseDate'].max() - ctm_max_purchase['MaxPurchaseDate']).dt.days\n",
    "    number_of_clusters = 3\n",
    "    recency_knn_cluster = KMeans(n_clusters=number_of_clusters)\n",
    "    recency_knn_cluster.fit(ctm_max_purchase[['Recency']])\n",
    "    ctm_max_purchase['RecencyCluster'] = recency_knn_cluster.predict(ctm_max_purchase[['Recency']])\n",
    "    order_df = pd.merge(order_df, ctm_max_purchase[['userID', 'RecencyCluster']], on='userID')\n",
    "    return order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFrequency(order_df_complete):\n",
    "    global dataframe_items_clusters_frequency\n",
    "    [order_df,item] = order_df_complete\n",
    "    ctm_frequency = order_df.groupby('userID').date.count().reset_index()\n",
    "    ctm_frequency.columns = ['userID', 'Frequency']\n",
    "    frequency_knn_cluster = KMeans(n_clusters=3)\n",
    "    frequency_knn_cluster.fit(ctm_frequency[['Frequency']])\n",
    "    ctm_frequency['FrequencyCluster'] = frequency_knn_cluster.predict(ctm_frequency[['Frequency']])\n",
    "    order_df = pd.merge(order_df, ctm_frequency[['userID', 'FrequencyCluster']], on='userID')\n",
    "    dataframe_items_clusters_frequency[item] = frequency_knn_cluster\n",
    "    return [order_df,item]\n",
    "def addCompleteFrequency(order_df):\n",
    "    ctm_frequency = order_df.groupby('userID').date.count().reset_index()\n",
    "    ctm_frequency.columns = ['userID', 'Frequency']\n",
    "    frequency_knn_cluster = KMeans(n_clusters=3)\n",
    "    frequency_knn_cluster.fit(ctm_frequency[['Frequency']])\n",
    "    ctm_frequency['FrequencyCluster'] = frequency_knn_cluster.predict(ctm_frequency[['Frequency']])\n",
    "    order_df = pd.merge(order_df, ctm_frequency[['userID', 'FrequencyCluster']], on='userID')\n",
    "    return order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addQuantity(order_df_complete):\n",
    "    [order_df,item] = order_df_complete\n",
    "    ctm_total_quantity = order_df.groupby('userID').order.sum().reset_index()\n",
    "    ctm_total_quantity.columns = ['userID','Quantity_Total']\n",
    "    quantity_knn_cluster = KMeans(n_clusters=3)\n",
    "    quantity_knn_cluster.fit(ctm_total_quantity[['Quantity_Total']])\n",
    "    ctm_total_quantity['Quantity_Cluster'] = quantity_knn_cluster.predict(ctm_total_quantity[['Quantity_Total']])\n",
    "    order_df = pd.merge(order_df, ctm_total_quantity[['userID', 'Quantity_Cluster']], on='userID')\n",
    "    global dataframe_items_clusters_quantity\n",
    "    dataframe_items_clusters_quantity[item] = quantity_knn_cluster\n",
    "    return [order_df,item]\n",
    "def addCompleteQuantity(order_df):\n",
    "    ctm_total_quantity = order_df.groupby('userID').order.sum().reset_index()\n",
    "    ctm_total_quantity.columns = ['userID','Quantity_Total']\n",
    "    quantity_knn_cluster = KMeans(n_clusters=3)\n",
    "    quantity_knn_cluster.fit(ctm_total_quantity[['Quantity_Total']])\n",
    "    ctm_total_quantity['Quantity_Cluster'] = quantity_knn_cluster.predict(ctm_total_quantity[['Quantity_Total']])\n",
    "    order_df = pd.merge(order_df, ctm_total_quantity[['userID', 'Quantity_Cluster']], on='userID')\n",
    "    return order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_variables():\n",
    "    global order_df_original,submission_df,order_df,order_df_split_1m,order_df_with_all_data\n",
    "    order_df_original = pd.read_csv('../src/data/orders_before_dec.csv',delimiter='|',parse_dates=True)\n",
    "    order_df_original['date'] = pd.to_datetime(order_df_original['date'], errors='coerce')\n",
    "    order_df_original['week'] = order_df_original.date.dt.isocalendar().week\n",
    "    order_df_original['week'] = order_df_original['week']%4\n",
    "    submission_df = pd.read_csv('../src/data/submission_dec.csv',delimiter='|',parse_dates=True)\n",
    "    order_df_with_all_data = addCompleteRecency(addCompleteQuantity(addCompleteFrequency(order_df_original)))\n",
    "set_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df_with_all_data.to_csv('completedataset_user_embedding_freq-rec-uency&qunatity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in submission_df.iterrows():\n",
    "    df1 = order_df_original[order_df_original['userID']==row['userID']]\n",
    "    df2 = df1[df1['itemID']==row['itemID']]\n",
    "    if df2.empty:\n",
    "        print(f\"user -{row['userID']} AND item-{row['itemID']} does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_onemonth(df):\n",
    "    df['month'] = df.date.dt.month\n",
    "    df_big = df[df['month']<df['date'].max().month]\n",
    "    df_small = df[df['month']==df['date'].max().month]\n",
    "    df_big.drop(['month'],1,inplace=True)\n",
    "    df_small.drop(['month'],1,inplace=True)\n",
    "    return df_big,df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_itemid_from_submission():\n",
    "    global submission_df\n",
    "    items = submission_df.itemID.unique()\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe_for_item(itemid):\n",
    "    global order_df_original,datafrane_items_realdataframe\n",
    "    temp_order_df = order_df_original[order_df_original['itemID']==itemid]\n",
    "    temp_order_df.drop(['itemID'],axis=1,inplace=True)\n",
    "    big_df ,small_df = split_df_onemonth(temp_order_df)\n",
    "    big_df = addRecency(addQuantity(addFrequency([big_df,itemid])))[0]\n",
    "    datafrane_items_realdataframe[itemid]=big_df\n",
    "    return big_df,small_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_y_label_to_dataframe(a,b):\n",
    "    #create a dataframe with customer id and first purchase date in tx_next\n",
    "    tx_next_first_purchase = b.groupby('userID').date.min().reset_index()\n",
    "    tx_next_first_purchase.columns = ['userID','MinPurchaseDate']\n",
    "\n",
    "    #create a dataframe with customer id and last purchase date in tx_5m\n",
    "    tx_last_purchase = a.groupby('userID').date.max().reset_index()\n",
    "    tx_last_purchase.columns = ['userID','MaxPurchaseDate']\n",
    "\n",
    "    #merge two dataframes\n",
    "    tx_purchase_dates = pd.merge(tx_last_purchase,tx_next_first_purchase,on='userID',how='left')\n",
    "\n",
    "    #calculate the time difference in days:\n",
    "    tx_purchase_dates['NextPurchaseDay'] = (tx_purchase_dates['MinPurchaseDate'] - tx_purchase_dates['MaxPurchaseDate']).dt.days\n",
    "    tx_user = pd.DataFrame(a['userID'].unique())\n",
    "    tx_user.columns = ['userID']\n",
    "    #merge with tx_user \n",
    "    tx_user = pd.merge(tx_user, tx_purchase_dates[['userID','NextPurchaseDay']],on='userID',how='left')\n",
    "    a = pd.merge(a,tx_user,on='userID')\n",
    "    a['NextPurchaseDayRange'] = 1\n",
    "    a.loc[a.NextPurchaseDay>7,'NextPurchaseDayRange'] = 2\n",
    "    a.loc[a.NextPurchaseDay>14,'NextPurchaseDayRange'] = 3\n",
    "    a.loc[a.NextPurchaseDay>21,'NextPurchaseDayRange'] = 4\n",
    "    a.loc[a.NextPurchaseDay>28,'NextPurchaseDayRange'] = 0\n",
    "    a.drop(['NextPurchaseDay'],1,inplace=True)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_test_set(a):\n",
    "    X, y = a.drop('NextPurchaseDayRange',axis=1), a.NextPurchaseDayRange\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    return X,y,X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model(model,X,y):\n",
    "    if model == 'LR':\n",
    "        reg = LogisticRegression().fit(X, y)\n",
    "    if model == 'NB':\n",
    "        reg = GaussianNB().fit(X, y)\n",
    "    if model == 'RF':\n",
    "        reg = RandomForestClassifier().fit(X, y)\n",
    "    if model == 'SVC':\n",
    "        reg = SVC().fit(X, y)\n",
    "    if model == 'Dtree':\n",
    "        reg = DecisionTreeClassifier().fit(X, y)\n",
    "    if model == 'XGB':\n",
    "        reg = xgb.XGBClassifier(enable_categorical=True).fit(X, y)\n",
    "    if model == 'KNN':\n",
    "        reg = KNeighborsClassifier().fit(X, y)\n",
    "    return reg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(X,y,X_train, X_test, y_train, y_test):\n",
    "    dic = {}\n",
    "    global models\n",
    "    for name,model in models:\n",
    "        kfold = KFold(n_splits=2)\n",
    "        cv_result = cross_val_score(model,X,y, cv = kfold,scoring = \"accuracy\")\n",
    "        if cv_result[0]!=math.nan :\n",
    "            dic[name]=cv_result[0]\n",
    "        # print(name, cv_result[0])\n",
    "    if len(dic) != 0:\n",
    "        bestmodel = max(dic, key=dic.get)\n",
    "        return train_best_model(bestmodel,X,y),bestmodel\n",
    "    else:\n",
    "        return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_representation_from_complete(userid,itemid):\n",
    "    global order_df_with_all_data\n",
    "    df = order_df_with_all_data[order_df_with_all_data['userID']==userid].drop(['userID','date','week','itemID'],1).iloc[0]\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_representation_from_item(userid,itemid):\n",
    "    global datafrane_items_realdataframe\n",
    "    p = datafrane_items_realdataframe[itemid]\n",
    "    df = p[p['userID']==userid].drop(['userID','date','week'],1).iloc[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_for_each_item():\n",
    "    for item in get_itemid_from_submission():\n",
    "        try:\n",
    "            a,b=build_dataframe_for_item(item)\n",
    "            a = add_y_label_to_dataframe(a,b)\n",
    "            X,y,X_train, X_test, y_train, y_test = build_train_test_set(a)\n",
    "            model,bestmodel = find_best_model(X.drop(['userID','date','week'],1),y,X_train.drop(['userID','date','week'],1), X_test.drop(['userID','date','week'],1), y_train, y_test)\n",
    "            if model == None:\n",
    "                dataframe_items[item] = 'no best model'\n",
    "            else:\n",
    "                dataframe_items[item] = [bestmodel,model]\n",
    "        except: \n",
    "            dataframe_items[item] = 'error in splitting data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill\n",
    "# dill.dump_session('notebook_env.db')\n",
    "# dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_csv_with_complete_data():\n",
    "    get_models_for_each_item()\n",
    "    with open(r\"mysub_complete.csv\", \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter =\"|\",quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"userID\",\"itemID\",\"prediction\"])\n",
    "        for idx,row in submission_df.iterrows():\n",
    "            user_rep = get_user_representation_from_complete(row['userID'],row['itemID'])\n",
    "            c = dataframe_items[int(row['itemID'])][1]\n",
    "            if type(c)==str:\n",
    "                writer.writerow([int(row['userID']),int(row['itemID']),\"None\"])\n",
    "            else:\n",
    "                try:\n",
    "                    d = c.predict([user_rep])\n",
    "                    writer.writerow([int(row['userID']),int(row['itemID']),d[0]])\n",
    "                except:\n",
    "                    print(f\"error in {dataframe_items[int(row['itemID'])][0]} \")\n",
    "                    writer.writerow([row['userID'],row['itemID'],\"INTERNAL ERROR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_csv_from_item_data():\n",
    "    #get_models_for_each_item()\n",
    "    with open(r\"mysub_item.csv\", \"w\") as csv_file:\n",
    "        writer = csv.writer(csv_file, delimiter =\"|\",quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([\"userID\",\"itemID\",\"prediction\"])\n",
    "        for idx,row in submission_df.iterrows():\n",
    "            try:\n",
    "                user_rep = get_user_representation_from_item(row['userID'],row['itemID'])\n",
    "            except:\n",
    "                print(f\"User rep does not exist for {row['itemID']}\")\n",
    "                writer.writerow([row['userID'],row['itemID'],\"User rep couldnt be found\"])\n",
    "            c = dataframe_items[int(row['itemID'])][1]\n",
    "            if type(c)==str:\n",
    "                writer.writerow([int(row['userID']),int(row['itemID']),\"None\"])\n",
    "            else:\n",
    "                try:\n",
    "                    d = c.predict([user_rep])\n",
    "                    writer.writerow([int(row['userID']),int(row['itemID']),d[0]])\n",
    "                except:\n",
    "                    print(f\"error in {dataframe_items[int(row['itemID'])][0]} \")\n",
    "                    writer.writerow([row['userID'],row['itemID'],\"Model doesnt exist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_csv_with_complete_data()\n",
    "fill_csv_from_item_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_sub_dec =  pd.read_csv('mysub_complete_dec.csv',delimiter='|')\n",
    "item_sub_dec =  pd.read_csv('mysub_item_dec.csv',delimiter='|')\n",
    "com_sub_jan =  pd.read_csv('mysub_fromcomplete_jan.csv',delimiter='|')\n",
    "item_sub_jan =  pd.read_csv('mysub_fromitem_jan.csv',delimiter='|')\n",
    "dec_pujit =  pd.read_csv('submission_dec_01_pujit.csv',delimiter='|')\n",
    "jan_pujit =  pd.read_csv('submission_jan_01_pujit.csv',delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sub_dec = item_sub_dec[item_sub_dec['prediction']!='User rep couldnt be found']\n",
    "item_sub_jan = item_sub_jan[item_sub_jan['prediction']!='User rep couldnt be found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sub_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_pujit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_sub_dec['prediction'].iloc[1] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in dec_pujit.iterrows():\n",
    "    try:\n",
    "        int(com_sub_dec.iloc[idx]['prediction'])\n",
    "    except:\n",
    "        com_sub_dec['prediction'].iloc[idx] = int(row['prediction'])\n",
    "    try:\n",
    "        int(item_sub_dec['prediction'].iloc[idx])\n",
    "    except:\n",
    "        item_sub_dec['prediction'].iloc[idx] = int(row['prediction'])\n",
    "\n",
    "for idx,row in jan_pujit.iterrows():\n",
    "    try:\n",
    "        int(com_sub_jan['prediction'].iloc[idx])\n",
    "    except:\n",
    "        com_sub_jan['prediction'].iloc[idx] = int(row['prediction'])\n",
    "    try:\n",
    "        int(item_sub_jan['prediction'].iloc[idx])\n",
    "    except:\n",
    "        item_sub_jan['prediction'].iloc[idx] = int(row['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in dec_pujit.iterrows():\n",
    "    if np.isnan(item_sub_dec.iloc[idx]['prediction']):\n",
    "        print('here')\n",
    "        item_sub_dec['prediction'].iloc[idx] = int(row['prediction'])\n",
    "for idx,row in jan_pujit.iterrows():\n",
    "    if np.isnan(item_sub_jan.iloc[idx]['prediction']):\n",
    "        item_sub_jan['prediction'].iloc[idx] = int(row['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in dec_pujit.iterrows():\n",
    "    print(item_sub_dec['prediction'].iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(com_sub_dec.prediction.unique())\n",
    "print(item_sub_dec.prediction.unique())\n",
    "print(com_sub_jan.prediction.unique())\n",
    "print(item_sub_jan.prediction.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_sub_dec['prediction']=com_sub_dec['prediction'].astype(int)\n",
    "com_sub_dec['userID']=com_sub_dec['userID'].astype(int)\n",
    "com_sub_dec['itemID']=com_sub_dec['itemID'].astype(int)\n",
    "\n",
    "com_sub_jan['prediction']=com_sub_jan['prediction'].astype(int)\n",
    "com_sub_jan['userID']=com_sub_jan['userID'].astype(int)\n",
    "com_sub_jan['itemID']=com_sub_jan['itemID'].astype(int)\n",
    "\n",
    "item_sub_dec['prediction']=item_sub_dec['prediction'].astype(int)\n",
    "item_sub_dec['userID']=item_sub_dec['userID'].astype(int)\n",
    "item_sub_dec['itemID']=item_sub_dec['itemID'].astype(int)\n",
    "\n",
    "item_sub_jan['prediction']=item_sub_jan['prediction'].astype(int)\n",
    "item_sub_jan['userID']=item_sub_jan['userID'].astype(int)\n",
    "item_sub_jan['itemID']=item_sub_jan['itemID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sub_jan['prediction']=pd.to_numeric(item_sub_jan['prediction'], errors='coerce')\n",
    "item_sub_jan['userID']=item_sub_jan['userID'].astype(int)\n",
    "item_sub_jan['itemID']=item_sub_jan['itemID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sub_dec['prediction']=pd.to_numeric(item_sub_dec['prediction'], errors='coerce')\n",
    "item_sub_dec['userID']=item_sub_dec['userID'].astype(int)\n",
    "item_sub_dec['itemID']=item_sub_dec['itemID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_sub_dec.to_csv('cd.csv',index=False,sep='|')\n",
    "com_sub_jan.to_csv('cj.csv',index=False,sep='|')\n",
    "item_sub_jan.to_csv('ij.csv',index=False,sep='|')\n",
    "item_sub_dec.to_csv('id.csv',index=False,sep='|')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
